{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity Score on the MovieLens Dataset\n",
    "\n",
    "In this notebook, we look at measuring diversity of movie consumption in the MovieLens dataset. This dataset comes from the interaction between users and movies at [MovieLens](https://movielens.org/), a public movie recommendation service run by [GroupLens](https://grouplens.org/), based at the University of Minesotta.\n",
    "\n",
    "Our goal is to try Spotify's diversity measure on the MovieLens dataset. This will require training a MovieLens model on the dataset, followed by computing the Generalist-Specialist metric on users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging  \n",
    "import multiprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core.util import *\n",
    "from core.metrics import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\"size\" : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download and Preparation\n",
    "\n",
    "First we download the dataset into the current working directory and unzip it. We also set up the paths to the user interaction with movies and ratings they gave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir_name = \"ml-latest\"\n",
    "base_path = os.path.join(os.getcwd(), dataset_dir_name)\n",
    "ratings_path = os.path.join(base_path, \"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277M/277M [01:10<00:00, 3.95MiB/s]\n",
      "INFO - 21:20:28: unzipping file\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(base_path):\n",
    "    dataset_url = \"http://files.grouplens.org/datasets/movielens/\" + dataset_dir_name + \".zip\"\n",
    "    # don't download if not necessary     \n",
    "    if not os.path.exists(base_path + \".zip\"):\n",
    "        download_dataset(dataset_url, base_path + \".zip\")\n",
    "    \n",
    "    logger.info(\"unzipping file\")\n",
    "    unzip_file(dataset_dir_name + \".zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Explore Dataset\n",
    "\n",
    "Note that unlike in Spotify, since the total number of users here are a lot smaller than at Spotify, we opt to train the model on the entire dataset. We have to convert the `movidId` column to `str` from `int` because the model accepts only a sequence of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(ratings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"date\"] = pd.to_datetime(ratings_df[\"timestamp\"], unit=\"s\")\n",
    "ratings_df[\"month\"] = ratings_df[\"date\"].dt.month\n",
    "ratings_df[\"year\"] = ratings_df[\"date\"].dt.year\n",
    "ratings_df[\"movieId\"] = ratings_df[\"movieId\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_movies = ratings_df.sort_values(by=[\"timestamp\"]).groupby(\n",
    "    \"userId\", \n",
    "    as_index=False\n",
    ").apply(\n",
    "    lambda x: dict(zip(x[\"movieId\"], x[\"rating\"]))\n",
    ").reset_index(name=\"movie_id_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'3826': 2.0, '307': 3.5, '1590': 2.5, '2478':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'1962': 4.0, '849': 3.5, '2108': 3.5, '2746':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'1321': 4.0, '960': 3.0, '1221': 4.0, '3171':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'2683': 3.5, '2997': 4.0, '786': 4.0, '1527':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'1186': 2.0, '2485': 3.0, '728': 3.0, '3178':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                   movie_id_ratings\n",
       "0      0  {'3826': 2.0, '307': 3.5, '1590': 2.5, '2478':...\n",
       "1      1  {'1962': 4.0, '849': 3.5, '2108': 3.5, '2746':...\n",
       "2      2  {'1321': 4.0, '960': 3.0, '1221': 4.0, '3171':...\n",
       "3      3  {'2683': 3.5, '2997': 4.0, '786': 4.0, '1527':...\n",
       "4      4  {'1186': 2.0, '2485': 3.0, '728': 3.0, '3178':..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_distribution = users_movies[\"movie_id_ratings\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see some statistics about the dataset. The ratings start from 1995 and lasted till 2018. Moreover, the number of movies watched and rated follows a power-law distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:47:23: Earliest year: 1995\n",
      "INFO - 23:47:23: Latest year: 2018\n",
      "INFO - 23:47:23: Number of users: 283228\n",
      "INFO - 23:47:23: Minimum number of movies watched and rated: 1\n",
      "INFO - 23:47:23: Median number of movies watched and rated: 30.0\n",
      "INFO - 23:47:23: Average number of movies watched and rated: 97.99\n",
      "INFO - 23:47:23: Maximum number of movies watched and rated: 23715\n"
     ]
    }
   ],
   "source": [
    "min_year = ratings_df[\"year\"].min()\n",
    "max_year = ratings_df[\"year\"].max()\n",
    "max_sequence_length = sequence_distribution.max()\n",
    "min_sequence_length = sequence_distribution.min()\n",
    "median_sequence_length = sequence_distribution.median()\n",
    "avg_sequence_length = sequence_distribution.mean()\n",
    "logger.info(f\"Earliest year: {min_year}\")\n",
    "logger.info(f\"Latest year: {max_year}\")\n",
    "logger.info(f\"Number of users: {sequence_distribution.shape[0]}\")\n",
    "logger.info(f\"Minimum number of movies watched and rated: {min_sequence_length}\")\n",
    "logger.info(f\"Median number of movies watched and rated: {median_sequence_length}\")\n",
    "logger.info(f\"Average number of movies watched and rated: {round(avg_sequence_length, 2)}\")\n",
    "logger.info(f\"Maximum number of movies watched and rated: {max_sequence_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Here, we train the Word2Vec model. We set our model parameters to \n",
    "* item vector size at 50 dimensions, \n",
    "* window size of 5, and\n",
    "* the minimum item count is 5, which means a movie needs to appear at least 5 times before it is included in the model.\n",
    "All other parameters follow the default parameter values specified in the GenSim docs.\n",
    "\n",
    "We use the **continuous bag-of-words** training method here. Most sequences are not very long, so a window size of length 5 is reasonable. **Note**: we have not performed any hyperparameter optimization in this notebook, our goal is to demonstrate the diversity scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "ITEM_VECTOR_SIZE = 60  # dimension of word vector size (multiple of 4 for best performance)\n",
    "MIN_ITEM_COUNT = 5 # minimum number of times a word must appear in the dataset to be included as a word vector\n",
    "WINDOW_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_list = users_movies[\"movie_id_ratings\"].apply(lambda x: list(x.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model = Word2Vec(min_count=MIN_ITEM_COUNT,\n",
    "                     window=WINDOW_SIZE,\n",
    "                     size=ITEM_VECTOR_SIZE,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:21:33: collecting all words and their counts\n",
      "INFO - 16:21:33: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #10000, processed 994882 words, keeping 22011 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #20000, processed 1955233 words, keeping 25926 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #30000, processed 2901806 words, keeping 28887 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #40000, processed 3879652 words, keeping 31329 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #50000, processed 4873537 words, keeping 33318 word types\n",
      "INFO - 16:21:33: PROGRESS: at sentence #60000, processed 5833186 words, keeping 34816 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #70000, processed 6816332 words, keeping 36633 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #80000, processed 7772985 words, keeping 38394 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #90000, processed 8731983 words, keeping 39544 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #100000, processed 9690383 words, keeping 40386 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #110000, processed 10700180 words, keeping 41619 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #120000, processed 11699439 words, keeping 42722 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #130000, processed 12694307 words, keeping 46298 word types\n",
      "INFO - 16:21:34: PROGRESS: at sentence #140000, processed 13691886 words, keeping 47180 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #150000, processed 14666393 words, keeping 48033 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #160000, processed 15666124 words, keeping 48608 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #170000, processed 16617490 words, keeping 49021 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #180000, processed 17605088 words, keeping 49825 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #190000, processed 18631149 words, keeping 50309 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #200000, processed 19601889 words, keeping 50998 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #210000, processed 20569074 words, keeping 51244 word types\n",
      "INFO - 16:21:35: PROGRESS: at sentence #220000, processed 21529507 words, keeping 51688 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #230000, processed 22485253 words, keeping 51943 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #240000, processed 23482147 words, keeping 52419 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #250000, processed 24450812 words, keeping 52689 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #260000, processed 25451195 words, keeping 52992 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #270000, processed 26441272 words, keeping 53466 word types\n",
      "INFO - 16:21:36: PROGRESS: at sentence #280000, processed 27439102 words, keeping 53783 word types\n",
      "INFO - 16:21:36: collected 53889 word types from a corpus of 27753444 raw words and 283228 sentences\n",
      "INFO - 16:21:36: Loading a fresh vocabulary\n",
      "INFO - 16:21:37: effective_min_count=5 retains 30824 unique words (57% of original 53889, drops 23065)\n",
      "INFO - 16:21:37: effective_min_count=5 leaves 27708216 word corpus (99% of original 27753444, drops 45228)\n",
      "INFO - 16:21:37: deleting the raw counts dictionary of 53889 items\n",
      "INFO - 16:21:37: sample=0.001 downsamples 7 most-common words\n",
      "INFO - 16:21:37: downsampling leaves estimated 27629970 word corpus (99.7% of prior 27708216)\n",
      "INFO - 16:21:37: estimated required memory for 30824 words and 60 dimensions: 30207520 bytes\n",
      "INFO - 16:21:37: resetting layer weights\n",
      "INFO - 16:21:42: Time to build vocab: 0.16 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "model.build_vocab(movies_list, progress_per=10000)\n",
    "logger.info(\"Time to build vocab: {} mins\".format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:21:42: training model with 11 workers on 30824 vocabulary and 60 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 16:21:43: EPOCH 1 - PROGRESS: at 10.47% examples, 2850170 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:44: EPOCH 1 - PROGRESS: at 21.05% examples, 2861682 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:21:45: EPOCH 1 - PROGRESS: at 31.73% examples, 2871038 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:46: EPOCH 1 - PROGRESS: at 41.91% examples, 2859931 words/s, in_qsize 22, out_qsize 2\n",
      "INFO - 16:21:47: EPOCH 1 - PROGRESS: at 52.53% examples, 2864713 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:21:48: EPOCH 1 - PROGRESS: at 62.86% examples, 2867881 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:49: EPOCH 1 - PROGRESS: at 73.28% examples, 2872269 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:50: EPOCH 1 - PROGRESS: at 83.95% examples, 2875442 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:51: EPOCH 1 - PROGRESS: at 94.33% examples, 2876773 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:21:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:21:52: EPOCH - 1 : training on 27753444 raw words (27622333 effective words) took 9.6s, 2878472 effective words/s\n",
      "INFO - 16:21:53: EPOCH 2 - PROGRESS: at 10.49% examples, 2840523 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:54: EPOCH 2 - PROGRESS: at 21.15% examples, 2878481 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:21:55: EPOCH 2 - PROGRESS: at 31.89% examples, 2895065 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:21:56: EPOCH 2 - PROGRESS: at 42.32% examples, 2889929 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:21:57: EPOCH 2 - PROGRESS: at 52.83% examples, 2887369 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:58: EPOCH 2 - PROGRESS: at 63.13% examples, 2881991 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:21:59: EPOCH 2 - PROGRESS: at 73.65% examples, 2888254 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:00: EPOCH 2 - PROGRESS: at 84.40% examples, 2886897 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:22:01: EPOCH 2 - PROGRESS: at 94.81% examples, 2888472 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:01: EPOCH - 2 : training on 27753444 raw words (27621915 effective words) took 9.6s, 2891136 effective words/s\n",
      "INFO - 16:22:02: EPOCH 3 - PROGRESS: at 10.43% examples, 2841859 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:03: EPOCH 3 - PROGRESS: at 20.83% examples, 2848119 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:04: EPOCH 3 - PROGRESS: at 31.37% examples, 2858977 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:05: EPOCH 3 - PROGRESS: at 41.71% examples, 2867456 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:06: EPOCH 3 - PROGRESS: at 52.16% examples, 2868262 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:07: EPOCH 3 - PROGRESS: at 62.51% examples, 2870723 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:09: EPOCH 3 - PROGRESS: at 72.71% examples, 2866919 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:10: EPOCH 3 - PROGRESS: at 83.43% examples, 2867270 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:22:11: EPOCH 3 - PROGRESS: at 93.94% examples, 2871764 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:11: EPOCH - 3 : training on 27753444 raw words (27621624 effective words) took 9.6s, 2873721 effective words/s\n",
      "INFO - 16:22:12: EPOCH 4 - PROGRESS: at 10.28% examples, 2800376 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:13: EPOCH 4 - PROGRESS: at 21.05% examples, 2879953 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:14: EPOCH 4 - PROGRESS: at 31.60% examples, 2871769 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:22:15: EPOCH 4 - PROGRESS: at 41.95% examples, 2868575 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:16: EPOCH 4 - PROGRESS: at 52.39% examples, 2869371 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:17: EPOCH 4 - PROGRESS: at 62.65% examples, 2867254 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:18: EPOCH 4 - PROGRESS: at 72.86% examples, 2865079 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:22:19: EPOCH 4 - PROGRESS: at 83.66% examples, 2870215 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:22:20: EPOCH 4 - PROGRESS: at 94.13% examples, 2870629 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:21: EPOCH - 4 : training on 27753444 raw words (27621879 effective words) took 9.6s, 2874156 effective words/s\n",
      "INFO - 16:22:22: EPOCH 5 - PROGRESS: at 10.49% examples, 2831728 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:22:23: EPOCH 5 - PROGRESS: at 21.14% examples, 2865637 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:24: EPOCH 5 - PROGRESS: at 31.94% examples, 2890090 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:25: EPOCH 5 - PROGRESS: at 42.20% examples, 2884642 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:26: EPOCH 5 - PROGRESS: at 52.60% examples, 2879600 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:27: EPOCH 5 - PROGRESS: at 63.02% examples, 2883105 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:28: EPOCH 5 - PROGRESS: at 73.34% examples, 2881451 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:29: EPOCH 5 - PROGRESS: at 83.95% examples, 2878899 words/s, in_qsize 20, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:22:30: EPOCH 5 - PROGRESS: at 94.37% examples, 2877767 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:30: EPOCH - 5 : training on 27753444 raw words (27622237 effective words) took 9.6s, 2881470 effective words/s\n",
      "INFO - 16:22:31: EPOCH 6 - PROGRESS: at 10.69% examples, 2902106 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:32: EPOCH 6 - PROGRESS: at 21.39% examples, 2906362 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:33: EPOCH 6 - PROGRESS: at 31.78% examples, 2882206 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:22:34: EPOCH 6 - PROGRESS: at 42.16% examples, 2882334 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:22:35: EPOCH 6 - PROGRESS: at 52.70% examples, 2880946 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:36: EPOCH 6 - PROGRESS: at 62.96% examples, 2876863 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:37: EPOCH 6 - PROGRESS: at 73.43% examples, 2879574 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:38: EPOCH 6 - PROGRESS: at 84.19% examples, 2884798 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:39: EPOCH 6 - PROGRESS: at 94.58% examples, 2885062 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:40: EPOCH - 6 : training on 27753444 raw words (27621612 effective words) took 9.6s, 2887718 effective words/s\n",
      "INFO - 16:22:41: EPOCH 7 - PROGRESS: at 10.31% examples, 2814966 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:42: EPOCH 7 - PROGRESS: at 20.93% examples, 2861715 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:43: EPOCH 7 - PROGRESS: at 31.37% examples, 2858245 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:44: EPOCH 7 - PROGRESS: at 41.74% examples, 2869502 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:45: EPOCH 7 - PROGRESS: at 52.13% examples, 2865564 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:46: EPOCH 7 - PROGRESS: at 62.33% examples, 2859348 words/s, in_qsize 17, out_qsize 4\n",
      "INFO - 16:22:47: EPOCH 7 - PROGRESS: at 72.68% examples, 2862243 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:48: EPOCH 7 - PROGRESS: at 83.26% examples, 2859181 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:49: EPOCH 7 - PROGRESS: at 93.47% examples, 2858019 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:50: EPOCH - 7 : training on 27753444 raw words (27621376 effective words) took 9.7s, 2860229 effective words/s\n",
      "INFO - 16:22:51: EPOCH 8 - PROGRESS: at 10.54% examples, 2858685 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:22:52: EPOCH 8 - PROGRESS: at 21.14% examples, 2887916 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:53: EPOCH 8 - PROGRESS: at 31.70% examples, 2872471 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:54: EPOCH 8 - PROGRESS: at 42.05% examples, 2876646 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:55: EPOCH 8 - PROGRESS: at 52.60% examples, 2878262 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:56: EPOCH 8 - PROGRESS: at 63.08% examples, 2879438 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:57: EPOCH 8 - PROGRESS: at 73.46% examples, 2877735 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:22:58: EPOCH 8 - PROGRESS: at 84.04% examples, 2874207 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:59: EPOCH 8 - PROGRESS: at 94.43% examples, 2876613 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:22:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:22:59: EPOCH - 8 : training on 27753444 raw words (27621588 effective words) took 9.6s, 2878246 effective words/s\n",
      "INFO - 16:23:00: EPOCH 9 - PROGRESS: at 10.31% examples, 2809446 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:01: EPOCH 9 - PROGRESS: at 21.05% examples, 2877642 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:02: EPOCH 9 - PROGRESS: at 31.55% examples, 2872236 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:23:03: EPOCH 9 - PROGRESS: at 42.01% examples, 2886137 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:04: EPOCH 9 - PROGRESS: at 52.32% examples, 2873997 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:05: EPOCH 9 - PROGRESS: at 62.65% examples, 2873193 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:06: EPOCH 9 - PROGRESS: at 73.11% examples, 2879971 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:07: EPOCH 9 - PROGRESS: at 83.66% examples, 2875803 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:08: EPOCH 9 - PROGRESS: at 93.85% examples, 2869672 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:09: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:23:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:09: EPOCH - 9 : training on 27753444 raw words (27622355 effective words) took 9.6s, 2871067 effective words/s\n",
      "INFO - 16:23:10: EPOCH 10 - PROGRESS: at 10.47% examples, 2832298 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:11: EPOCH 10 - PROGRESS: at 21.08% examples, 2875467 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:12: EPOCH 10 - PROGRESS: at 31.60% examples, 2857094 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:13: EPOCH 10 - PROGRESS: at 41.95% examples, 2866392 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:14: EPOCH 10 - PROGRESS: at 52.21% examples, 2857498 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:15: EPOCH 10 - PROGRESS: at 62.47% examples, 2857683 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:16: EPOCH 10 - PROGRESS: at 72.86% examples, 2859139 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:17: EPOCH 10 - PROGRESS: at 83.70% examples, 2866091 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:18: EPOCH 10 - PROGRESS: at 93.94% examples, 2865001 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:23:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:18: EPOCH - 10 : training on 27753444 raw words (27621980 effective words) took 9.6s, 2870472 effective words/s\n",
      "INFO - 16:23:19: EPOCH 11 - PROGRESS: at 10.12% examples, 2754087 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:20: EPOCH 11 - PROGRESS: at 20.83% examples, 2843083 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:21: EPOCH 11 - PROGRESS: at 31.29% examples, 2849281 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:23:22: EPOCH 11 - PROGRESS: at 41.67% examples, 2859054 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:23: EPOCH 11 - PROGRESS: at 52.16% examples, 2864939 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:24: EPOCH 11 - PROGRESS: at 62.51% examples, 2865026 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:25: EPOCH 11 - PROGRESS: at 72.86% examples, 2868949 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:26: EPOCH 11 - PROGRESS: at 83.62% examples, 2868682 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:27: EPOCH 11 - PROGRESS: at 94.24% examples, 2878042 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:23:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:28: EPOCH - 11 : training on 27753444 raw words (27621741 effective words) took 9.6s, 2879315 effective words/s\n",
      "INFO - 16:23:29: EPOCH 12 - PROGRESS: at 10.31% examples, 2810908 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:30: EPOCH 12 - PROGRESS: at 20.88% examples, 2853266 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:31: EPOCH 12 - PROGRESS: at 31.55% examples, 2868883 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:32: EPOCH 12 - PROGRESS: at 41.81% examples, 2866882 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:33: EPOCH 12 - PROGRESS: at 52.20% examples, 2864546 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:34: EPOCH 12 - PROGRESS: at 62.55% examples, 2867200 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:35: EPOCH 12 - PROGRESS: at 72.89% examples, 2866197 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:36: EPOCH 12 - PROGRESS: at 83.52% examples, 2864946 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:23:37: EPOCH 12 - PROGRESS: at 93.79% examples, 2862854 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:23:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:38: EPOCH - 12 : training on 27753444 raw words (27621663 effective words) took 9.6s, 2864652 effective words/s\n",
      "INFO - 16:23:39: EPOCH 13 - PROGRESS: at 10.49% examples, 2861810 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:40: EPOCH 13 - PROGRESS: at 20.93% examples, 2861565 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:23:41: EPOCH 13 - PROGRESS: at 31.48% examples, 2863525 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:42: EPOCH 13 - PROGRESS: at 41.67% examples, 2857136 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:43: EPOCH 13 - PROGRESS: at 52.09% examples, 2859464 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:44: EPOCH 13 - PROGRESS: at 62.33% examples, 2852508 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:45: EPOCH 13 - PROGRESS: at 72.71% examples, 2857775 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:46: EPOCH 13 - PROGRESS: at 83.26% examples, 2856367 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:47: EPOCH 13 - PROGRESS: at 93.70% examples, 2858035 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:23:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:47: EPOCH - 13 : training on 27753444 raw words (27621807 effective words) took 9.6s, 2863317 effective words/s\n",
      "INFO - 16:23:48: EPOCH 14 - PROGRESS: at 10.24% examples, 2795033 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:49: EPOCH 14 - PROGRESS: at 20.79% examples, 2841420 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:50: EPOCH 14 - PROGRESS: at 31.32% examples, 2852631 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:51: EPOCH 14 - PROGRESS: at 41.61% examples, 2854458 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:52: EPOCH 14 - PROGRESS: at 51.98% examples, 2855818 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:53: EPOCH 14 - PROGRESS: at 62.18% examples, 2852627 words/s, in_qsize 21, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:23:54: EPOCH 14 - PROGRESS: at 72.59% examples, 2858146 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:23:55: EPOCH 14 - PROGRESS: at 83.15% examples, 2858118 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:56: EPOCH 14 - PROGRESS: at 93.32% examples, 2855508 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:23:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:23:57: EPOCH - 14 : training on 27753444 raw words (27621465 effective words) took 9.7s, 2858458 effective words/s\n",
      "INFO - 16:23:58: EPOCH 15 - PROGRESS: at 10.49% examples, 2834651 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:23:59: EPOCH 15 - PROGRESS: at 20.96% examples, 2838407 words/s, in_qsize 22, out_qsize 1\n",
      "INFO - 16:24:00: EPOCH 15 - PROGRESS: at 31.66% examples, 2860054 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:24:01: EPOCH 15 - PROGRESS: at 41.98% examples, 2868328 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:24:02: EPOCH 15 - PROGRESS: at 52.56% examples, 2868148 words/s, in_qsize 18, out_qsize 3\n",
      "INFO - 16:24:03: EPOCH 15 - PROGRESS: at 62.93% examples, 2872624 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:24:04: EPOCH 15 - PROGRESS: at 73.03% examples, 2863684 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 16:24:05: EPOCH 15 - PROGRESS: at 83.83% examples, 2866018 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 16:24:06: EPOCH 15 - PROGRESS: at 94.29% examples, 2869924 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:24:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:24:07: EPOCH - 15 : training on 27753444 raw words (27621862 effective words) took 9.6s, 2872825 effective words/s\n",
      "INFO - 16:24:07: training on a 416301660 raw words (414327437 effective words) took 144.5s, 2868258 effective words/s\n",
      "INFO - 16:24:07: Time to train the model: 2.41 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "model.train(movies_list, total_examples=model.corpus_count, epochs=15, report_delay=1)\n",
    "logger.info(\"Time to train the model: {} mins\".format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:24:07: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# since we're no longer going to train further\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:24:07: Corpus size: 283228\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Corpus size: {model.corpus_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Generalist-Specialist Score\n",
    "\n",
    "Now that we've trained the model, we're in a position to apply the Generalist-Specialist score to all users in MovieLens dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = users_movies[\"movie_id_ratings\"].apply(lambda x: generalist_specialist_score(model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gs_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gs_score'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4c7d2c56b3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gs_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generalist-Specialist score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of users\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gs_score'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "sns.distplot(users_movies[\"gs_score\"], kde=False, bins=100)\n",
    "plt.xlabel(\"Generalist-Specialist score\")\n",
    "plt.ylabel(\"Number of users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of users are **generalists** since they consume a wide variety of movies. There is a spike of users with a score of 1.0, but the majority of these users only watched and rated a single movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_movies[\"num_movies_rated\"] = users_movies[\"movie_id_ratings\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists = users_movies[(users_movies[\"gs_score\"] >= 0.90) & (users_movies[\"num_movies_rated\"] > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:42:10: Number of specialists with more than 1 rating: 857\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Number of specialists with more than 1 rating: {specialists.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Shannon Entropy\n",
    "\n",
    "We also want to compare the scores from using Shannon Entropy, another common measure of diversity per user. The higher the score, the more diverse in terms of user's movie consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_movies[\"shannon_entropy\"] = movies_list.apply(lambda x: compute_shannon_entropy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
